{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL.Image import LANCZOS\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles/lib/python',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python38.zip',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/anuj/.ipython',\n",
       " '/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loaders import Omniglotmix, MiniImageNet\n",
    "from data.taskers import gen_tasks\n",
    "from src.zoo.archs import EncoderNN, MatchingNetwork\n",
    "from src.zoo.matching_nets_utils import setup, logits, accuracy, inner_adapt_matching\n",
    "#from src.utils import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = list(range(1623))\n",
    "random.shuffle(classes)\n",
    "image_transforms = transforms.Compose([transforms.Resize(28, interpolation=LANCZOS),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    lambda x: 1.0 - x,\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks('omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', image_transforms=image_transforms, n_ways=5, k_shots=1, q_shots=15, classes=classes[:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "train_tasks.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks(dataname='miniimagenet', root='../../mini_imagenet', mode='train', n_ways=5, k_shots=1, q_shots=15, image_transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup(\n",
    "    'omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', 5, 1, 15, 5, 1, 15, 1, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_adapt_matching_here(task, loss, learner, n_ways, k_shots, q_shots, EPSILON, device):\n",
    "    \n",
    "    \n",
    "\n",
    "    preds = logits(queries=queries, support=support, EPSILON=EPSILON)\n",
    "    attention = (-preds).softmax(dim=1)\n",
    "\n",
    "    y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "\n",
    "    y = support_labels.unsqueeze(-1)\n",
    "    y_onehot = y_onehot.scatter(1, y, 1)\n",
    "\n",
    "    y_pred = torch.mm(attention, y_onehot.to(device))\n",
    "\n",
    "    # Calculated loss with negative log likelihood\n",
    "    # Clip predictions for numerical stability\n",
    "    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n",
    "    eval_loss = loss(clipped_y_pred.log(), queries_labels)\n",
    "    eval_acc = accuracy(clipped_y_pred, queries_labels)\n",
    "\n",
    "    return eval_loss, eval_acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = ttask\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "data = learner.encoder(data)\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation\n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(\n",
    "        k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, _, _ = learner.support_encoder(support.unsqueeze(1))\n",
    "support = support.squeeze(1)\n",
    "queries = learner.query_encoder(queries, support, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = logits(queries=queries, support=support, EPSILON=1e-8)\n",
    "attention = (-preds).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (75).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ae779934b203>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minner_adapt_matching_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-38-0878a49af8d8>\u001b[0m in \u001b[0;36minner_adapt_matching_here\u001b[0;34m(task, loss, learner, n_ways, k_shots, q_shots, EPSILON, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Clip predictions for numerical stability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mclipped_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2385\u001b[0m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (75)."
     ]
    }
   ],
   "source": [
    "inner_adapt_matching_here(ttask, nn.NLLLoss(), learner, 5, 1, 15, 1e-8, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "Expected input batch_size (5) to match target batch_size (75).",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-5fb9e3b7f250>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minner_adapt_matching\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mttask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNLLLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src/zoo/matching_nets_utils.py\u001b[0m in \u001b[0;36minner_adapt_matching\u001b[0;34m(task, loss, learner, n_ways, k_shots, q_shots, EPSILON, device)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mclipped_y_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEPSILON\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mEPSILON\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0meval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0meval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclipped_y_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0meval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_acc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2383\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2384\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2385\u001b[0m             \u001b[0;34m\"Expected input batch_size ({}) to match target batch_size ({}).\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2386\u001b[0m         )\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (5) to match target batch_size (75)."
     ]
    }
   ],
   "source": [
    "loss, acc = inner_adapt_matching(ttask, nn.NLLLoss(), learner, 5, 1, 15, 1e-8, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MatchingNetwork(1, (2,2), True,\n",
    "                 1, 64, 2, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = learner.encoder(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([80])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "n_ways = 5; k_shots = 1; q_shots = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = a, labels1\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation\n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(\n",
    "        k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "support.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'BidrectionalLSTM' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-aa3f71487375>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBidrectionalLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'BidrectionalLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "g = BidrectionalLSTM(1600, 1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, _, _ = learner.support_encoder(support.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = support.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.7751,  0.0515,  0.1438,  0.1257,  0.6151,  0.3853, -0.1882,  0.2459,\n",
       "          0.2943,  0.1744,  1.0263,  0.2978, -0.0356,  2.6314,  1.1487,  1.4413,\n",
       "         -0.3744,  0.4918,  0.3630,  1.3094, -0.5490,  0.1826,  1.0999,  2.8820,\n",
       "          0.2817, -0.2709,  1.4906,  0.2703,  0.0196, -0.3053,  0.7905,  1.4817,\n",
       "          0.3449,  1.4603,  2.4432,  0.1692,  0.5180,  0.0957,  0.4308, -0.1557,\n",
       "          0.0240,  0.5268,  0.8025,  2.8366,  0.5357,  0.4689,  0.8396,  0.1500,\n",
       "         -0.3126,  1.2549,  0.1347,  1.3268,  0.1980,  0.7349,  0.3823, -0.1134,\n",
       "          0.8832,  0.5702,  0.1782, -0.0140,  0.9304,  0.8235,  0.0473,  0.1978],\n",
       "        [ 0.5596,  0.0285,  0.4590,  0.4996,  0.5670,  0.3072, -0.3717,  0.5611,\n",
       "          0.3285,  0.1010,  1.1196,  0.5308,  0.1131,  1.1376,  0.4775,  0.6012,\n",
       "          0.0713,  0.5458, -0.0818,  1.1150, -0.3621,  0.0060,  0.6649,  1.3462,\n",
       "          1.1295, -0.2163,  0.2017,  0.5394,  0.1455,  0.0228,  0.9664, -0.0309,\n",
       "          0.5418,  0.7056, -0.0066,  1.0946,  0.6782, -0.0810,  0.3958,  0.0773,\n",
       "          0.4395,  0.1797,  0.9695,  1.0752,  0.4916,  0.1138,  1.6065,  0.8709,\n",
       "          0.0651,  0.9181,  0.0697,  0.9968,  0.0678,  0.3776,  0.3263, -0.2156,\n",
       "          1.1203,  1.0405, -0.0099, -0.0498,  0.2811, -0.0323, -0.0339,  0.4084],\n",
       "        [ 1.1760, -0.0082,  0.2100,  0.6024,  1.6296, -0.0202, -0.4094,  0.5697,\n",
       "          0.1512,  0.3487,  0.9572,  0.0425,  0.2352,  1.1906,  0.9451,  0.5855,\n",
       "         -0.3993,  0.4441,  0.1310,  1.0340, -0.3989,  0.2881,  1.2988,  0.3289,\n",
       "          0.6998, -0.0379,  1.2081,  0.5783,  0.2824, -0.1221,  1.5305,  2.2392,\n",
       "          0.4750,  1.5206,  2.6661, -0.3907,  1.1134,  0.0190,  0.1308, -0.1972,\n",
       "          1.0019,  0.4300,  0.1489,  2.0506,  0.1595,  0.7329,  0.3095,  0.1334,\n",
       "         -0.2972,  0.5652,  0.2342,  1.2297,  0.3165,  0.6752,  0.8199, -0.4177,\n",
       "          1.5252,  0.1571, -0.1219,  0.5415,  0.9081,  0.8554,  0.2162, -0.0670],\n",
       "        [ 0.7000,  0.0225,  0.3286,  0.2977,  0.5381,  0.6781,  0.3994,  1.2558,\n",
       "          0.2896,  0.3105,  0.6109,  0.7731, -0.0704,  0.9544,  0.4518,  0.8837,\n",
       "         -0.1829,  0.2384, -0.0952,  0.4249, -0.2034,  0.8726,  0.3262,  0.4564,\n",
       "          0.8796, -0.0742,  1.0895,  0.8480,  0.1958,  0.4979,  0.0892,  2.3887,\n",
       "          0.6313,  0.8296,  0.6805, -0.3387,  0.3189,  0.0972,  0.5506, -0.1152,\n",
       "         -0.1621, -0.0151,  0.7045,  1.7355,  0.1102,  0.1751,  0.3550,  0.9797,\n",
       "          0.3447,  0.2895,  0.1178,  0.4013,  0.1967,  0.7972,  0.2096,  0.0104,\n",
       "          1.1069,  0.3627,  0.4833,  0.6113,  0.5188,  1.4506,  0.2803,  0.8602],\n",
       "        [ 1.1449, -0.0127,  0.3411,  0.3801,  0.7903,  0.3791,  0.5821,  0.5525,\n",
       "          0.3863,  0.2841,  0.8514,  0.5326,  0.2558,  0.9031,  0.1566,  0.9601,\n",
       "          0.3416,  0.3118, -0.0579,  0.4994, -0.2418,  0.6423,  0.8457,  0.5195,\n",
       "          1.2952, -0.1167,  1.5518,  0.8368,  0.2479,  0.3032,  0.4134,  1.0231,\n",
       "          0.4449,  0.6612,  1.2626, -0.2739,  0.8766, -0.0295,  0.2863,  0.7915,\n",
       "          0.6008,  0.0998,  0.4748,  1.9088,  0.0243,  0.7625,  2.2178, -0.3740,\n",
       "         -0.1955,  0.7535,  0.1397,  0.4045,  0.1238,  0.5643,  0.6760, -0.1767,\n",
       "          1.1619,  1.3561, -0.1809,  0.3101,  0.0233,  0.5613,  0.1737,  0.1088]],\n",
       "       grad_fn=<SqueezeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'AttentionLSTM' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-892bda4fb38f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAttentionLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1600\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'AttentionLSTM' is not defined"
     ]
    }
   ],
   "source": [
    "f = AttentionLSTM(1600, 2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = learner.query_encoder(support, queries, 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75, 64])"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "preds = logits(queries=queries, support=support, EPSILON=1e-8)\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "attention = (-preds).softmax(dim=1)\n",
    "attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "y_onehot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = support_labels.unsqueeze(-1)\n",
    "y_onehot = y_onehot.scatter(1, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([2, 3, 1, 4, 0])"
      ]
     },
     "metadata": {},
     "execution_count": 58
    }
   ],
   "source": [
    "support_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1.],\n",
       "        [1., 0., 0., 0., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 59
    }
   ],
   "source": [
    "y_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2058, 0.2036, 0.1957, 0.2050, 0.1899],\n",
       "        [0.1956, 0.1976, 0.2166, 0.2037, 0.1865],\n",
       "        [0.2083, 0.1987, 0.2101, 0.1919, 0.1910],\n",
       "        [0.2055, 0.2053, 0.2081, 0.1932, 0.1879],\n",
       "        [0.2011, 0.1960, 0.2133, 0.2040, 0.1856],\n",
       "        [0.2077, 0.2002, 0.2032, 0.2023, 0.1865],\n",
       "        [0.1995, 0.2046, 0.2021, 0.2026, 0.1913],\n",
       "        [0.1996, 0.2077, 0.2058, 0.2057, 0.1813],\n",
       "        [0.1987, 0.1895, 0.2126, 0.2015, 0.1978],\n",
       "        [0.1997, 0.2014, 0.2123, 0.2060, 0.1806],\n",
       "        [0.2008, 0.2039, 0.2126, 0.2014, 0.1813],\n",
       "        [0.2009, 0.2024, 0.2079, 0.2039, 0.1848],\n",
       "        [0.2066, 0.2012, 0.2052, 0.1955, 0.1914],\n",
       "        [0.2069, 0.2022, 0.2057, 0.1962, 0.1890],\n",
       "        [0.2067, 0.2056, 0.1974, 0.2034, 0.1869],\n",
       "        [0.1924, 0.1941, 0.2099, 0.2090, 0.1944],\n",
       "        [0.1998, 0.2021, 0.1974, 0.2039, 0.1968],\n",
       "        [0.2090, 0.1884, 0.2082, 0.1982, 0.1963],\n",
       "        [0.1945, 0.1949, 0.2063, 0.2106, 0.1936],\n",
       "        [0.2008, 0.2026, 0.1980, 0.2023, 0.1963],\n",
       "        [0.1957, 0.1991, 0.2065, 0.2043, 0.1944],\n",
       "        [0.2048, 0.1909, 0.2012, 0.2027, 0.2005],\n",
       "        [0.2042, 0.2128, 0.1937, 0.1972, 0.1921],\n",
       "        [0.1984, 0.1856, 0.2082, 0.2000, 0.2078],\n",
       "        [0.1955, 0.1906, 0.2148, 0.2027, 0.1965],\n",
       "        [0.1996, 0.2044, 0.2059, 0.2101, 0.1800],\n",
       "        [0.2146, 0.2050, 0.2013, 0.1910, 0.1881],\n",
       "        [0.1971, 0.1922, 0.1979, 0.2131, 0.1998],\n",
       "        [0.1960, 0.1865, 0.2162, 0.2027, 0.1986],\n",
       "        [0.2003, 0.1923, 0.2018, 0.2120, 0.1936],\n",
       "        [0.1983, 0.1988, 0.1998, 0.2027, 0.2003],\n",
       "        [0.2094, 0.2124, 0.1949, 0.1948, 0.1885],\n",
       "        [0.1947, 0.1989, 0.2046, 0.2104, 0.1914],\n",
       "        [0.2060, 0.2026, 0.1924, 0.2024, 0.1967],\n",
       "        [0.1987, 0.2002, 0.1980, 0.2059, 0.1972],\n",
       "        [0.2056, 0.2146, 0.1921, 0.1958, 0.1919],\n",
       "        [0.2119, 0.1995, 0.1937, 0.1996, 0.1953],\n",
       "        [0.2094, 0.2045, 0.1958, 0.1985, 0.1918],\n",
       "        [0.2018, 0.1967, 0.2029, 0.2041, 0.1945],\n",
       "        [0.2000, 0.2028, 0.1929, 0.2053, 0.1990],\n",
       "        [0.2029, 0.2111, 0.1939, 0.1967, 0.1953],\n",
       "        [0.2048, 0.2153, 0.1907, 0.1967, 0.1925],\n",
       "        [0.2011, 0.2036, 0.1960, 0.2045, 0.1949],\n",
       "        [0.1963, 0.1920, 0.2110, 0.2050, 0.1957],\n",
       "        [0.2099, 0.2105, 0.1927, 0.1937, 0.1932],\n",
       "        [0.2165, 0.1982, 0.1968, 0.1899, 0.1986],\n",
       "        [0.2096, 0.1981, 0.1976, 0.1975, 0.1972],\n",
       "        [0.2176, 0.2043, 0.1967, 0.1872, 0.1941],\n",
       "        [0.2063, 0.2019, 0.1981, 0.1982, 0.1955],\n",
       "        [0.2126, 0.1881, 0.2048, 0.1950, 0.1994],\n",
       "        [0.2085, 0.1924, 0.2008, 0.1937, 0.2046],\n",
       "        [0.2099, 0.1917, 0.2087, 0.1887, 0.2010],\n",
       "        [0.2029, 0.2102, 0.1969, 0.2024, 0.1876],\n",
       "        [0.2037, 0.2031, 0.2038, 0.2011, 0.1883],\n",
       "        [0.2079, 0.2080, 0.1962, 0.1949, 0.1930],\n",
       "        [0.1998, 0.1934, 0.1968, 0.2077, 0.2023],\n",
       "        [0.2096, 0.1917, 0.2058, 0.1925, 0.2004],\n",
       "        [0.2089, 0.1945, 0.2046, 0.2010, 0.1910],\n",
       "        [0.2038, 0.2005, 0.2016, 0.2020, 0.1921],\n",
       "        [0.2100, 0.1966, 0.1983, 0.1946, 0.2006],\n",
       "        [0.2104, 0.2067, 0.1954, 0.1949, 0.1927],\n",
       "        [0.2107, 0.2045, 0.1992, 0.1920, 0.1935],\n",
       "        [0.2099, 0.2080, 0.1969, 0.1900, 0.1951],\n",
       "        [0.2140, 0.2026, 0.1998, 0.1887, 0.1950],\n",
       "        [0.2158, 0.2042, 0.1948, 0.1886, 0.1965],\n",
       "        [0.2098, 0.1978, 0.1996, 0.1944, 0.1984],\n",
       "        [0.2114, 0.2091, 0.1978, 0.1862, 0.1954],\n",
       "        [0.2086, 0.2032, 0.1990, 0.1877, 0.2015],\n",
       "        [0.2079, 0.2116, 0.2027, 0.1919, 0.1858],\n",
       "        [0.2110, 0.2093, 0.2006, 0.1911, 0.1880],\n",
       "        [0.2043, 0.2086, 0.2009, 0.1960, 0.1902],\n",
       "        [0.2111, 0.2061, 0.2021, 0.1880, 0.1926],\n",
       "        [0.2080, 0.2053, 0.1999, 0.1926, 0.1943],\n",
       "        [0.2084, 0.1986, 0.2002, 0.1911, 0.2017],\n",
       "        [0.2098, 0.2040, 0.1938, 0.1994, 0.1929]], grad_fn=<MmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "y_pred = torch.mm(attention, y_onehot.to(device))\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([75])"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "queries_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2058, 0.2036, 0.1957, 0.2050, 0.1899],\n",
       "        [0.1956, 0.1976, 0.2166, 0.2037, 0.1865],\n",
       "        [0.2083, 0.1987, 0.2101, 0.1919, 0.1910],\n",
       "        [0.2055, 0.2053, 0.2081, 0.1932, 0.1879],\n",
       "        [0.2011, 0.1960, 0.2133, 0.2040, 0.1856],\n",
       "        [0.2077, 0.2002, 0.2032, 0.2023, 0.1865],\n",
       "        [0.1995, 0.2046, 0.2021, 0.2026, 0.1913],\n",
       "        [0.1996, 0.2077, 0.2058, 0.2057, 0.1813],\n",
       "        [0.1987, 0.1895, 0.2126, 0.2015, 0.1978],\n",
       "        [0.1997, 0.2014, 0.2123, 0.2060, 0.1806],\n",
       "        [0.2008, 0.2039, 0.2126, 0.2014, 0.1813],\n",
       "        [0.2009, 0.2024, 0.2079, 0.2039, 0.1848],\n",
       "        [0.2066, 0.2012, 0.2052, 0.1955, 0.1914],\n",
       "        [0.2069, 0.2022, 0.2057, 0.1962, 0.1890],\n",
       "        [0.2067, 0.2056, 0.1974, 0.2034, 0.1869],\n",
       "        [0.1924, 0.1941, 0.2099, 0.2090, 0.1944],\n",
       "        [0.1998, 0.2021, 0.1974, 0.2039, 0.1968],\n",
       "        [0.2090, 0.1884, 0.2082, 0.1982, 0.1963],\n",
       "        [0.1945, 0.1949, 0.2063, 0.2106, 0.1936],\n",
       "        [0.2008, 0.2026, 0.1980, 0.2023, 0.1963],\n",
       "        [0.1957, 0.1991, 0.2065, 0.2043, 0.1944],\n",
       "        [0.2048, 0.1909, 0.2012, 0.2027, 0.2005],\n",
       "        [0.2042, 0.2128, 0.1937, 0.1972, 0.1921],\n",
       "        [0.1984, 0.1856, 0.2082, 0.2000, 0.2078],\n",
       "        [0.1955, 0.1906, 0.2148, 0.2027, 0.1965],\n",
       "        [0.1996, 0.2044, 0.2059, 0.2101, 0.1800],\n",
       "        [0.2146, 0.2050, 0.2013, 0.1910, 0.1881],\n",
       "        [0.1971, 0.1922, 0.1979, 0.2131, 0.1998],\n",
       "        [0.1960, 0.1865, 0.2162, 0.2027, 0.1986],\n",
       "        [0.2003, 0.1923, 0.2018, 0.2120, 0.1936],\n",
       "        [0.1983, 0.1988, 0.1998, 0.2027, 0.2003],\n",
       "        [0.2094, 0.2124, 0.1949, 0.1948, 0.1885],\n",
       "        [0.1947, 0.1989, 0.2046, 0.2104, 0.1914],\n",
       "        [0.2060, 0.2026, 0.1924, 0.2024, 0.1967],\n",
       "        [0.1987, 0.2002, 0.1980, 0.2059, 0.1972],\n",
       "        [0.2056, 0.2146, 0.1921, 0.1958, 0.1919],\n",
       "        [0.2119, 0.1995, 0.1937, 0.1996, 0.1953],\n",
       "        [0.2094, 0.2045, 0.1958, 0.1985, 0.1918],\n",
       "        [0.2018, 0.1967, 0.2029, 0.2041, 0.1945],\n",
       "        [0.2000, 0.2028, 0.1929, 0.2053, 0.1990],\n",
       "        [0.2029, 0.2111, 0.1939, 0.1967, 0.1953],\n",
       "        [0.2048, 0.2153, 0.1907, 0.1967, 0.1925],\n",
       "        [0.2011, 0.2036, 0.1960, 0.2045, 0.1949],\n",
       "        [0.1963, 0.1920, 0.2110, 0.2050, 0.1957],\n",
       "        [0.2099, 0.2105, 0.1927, 0.1937, 0.1932],\n",
       "        [0.2165, 0.1982, 0.1968, 0.1899, 0.1986],\n",
       "        [0.2096, 0.1981, 0.1976, 0.1975, 0.1972],\n",
       "        [0.2176, 0.2043, 0.1967, 0.1872, 0.1941],\n",
       "        [0.2063, 0.2019, 0.1981, 0.1982, 0.1955],\n",
       "        [0.2126, 0.1881, 0.2048, 0.1950, 0.1994],\n",
       "        [0.2085, 0.1924, 0.2008, 0.1937, 0.2046],\n",
       "        [0.2099, 0.1917, 0.2087, 0.1887, 0.2010],\n",
       "        [0.2029, 0.2102, 0.1969, 0.2024, 0.1876],\n",
       "        [0.2037, 0.2031, 0.2038, 0.2011, 0.1883],\n",
       "        [0.2079, 0.2080, 0.1962, 0.1949, 0.1930],\n",
       "        [0.1998, 0.1934, 0.1968, 0.2077, 0.2023],\n",
       "        [0.2096, 0.1917, 0.2058, 0.1925, 0.2004],\n",
       "        [0.2089, 0.1945, 0.2046, 0.2010, 0.1910],\n",
       "        [0.2038, 0.2005, 0.2016, 0.2020, 0.1921],\n",
       "        [0.2100, 0.1966, 0.1983, 0.1946, 0.2006],\n",
       "        [0.2104, 0.2067, 0.1954, 0.1949, 0.1927],\n",
       "        [0.2107, 0.2045, 0.1992, 0.1920, 0.1935],\n",
       "        [0.2099, 0.2080, 0.1969, 0.1900, 0.1951],\n",
       "        [0.2140, 0.2026, 0.1998, 0.1887, 0.1950],\n",
       "        [0.2158, 0.2042, 0.1948, 0.1886, 0.1965],\n",
       "        [0.2098, 0.1978, 0.1996, 0.1944, 0.1984],\n",
       "        [0.2114, 0.2091, 0.1978, 0.1862, 0.1954],\n",
       "        [0.2086, 0.2032, 0.1990, 0.1877, 0.2015],\n",
       "        [0.2079, 0.2116, 0.2027, 0.1919, 0.1858],\n",
       "        [0.2110, 0.2093, 0.2006, 0.1911, 0.1880],\n",
       "        [0.2043, 0.2086, 0.2009, 0.1960, 0.1902],\n",
       "        [0.2111, 0.2061, 0.2021, 0.1880, 0.1926],\n",
       "        [0.2080, 0.2053, 0.1999, 0.1926, 0.1943],\n",
       "        [0.2084, 0.1986, 0.2002, 0.1911, 0.2017],\n",
       "        [0.2098, 0.2040, 0.1938, 0.1994, 0.1929]], grad_fn=<ClampBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "clipped_y_pred = y_pred.clamp(1e-8, 1 - 1e-8)\n",
    "clipped_y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.NLLLoss()\n",
    "eval_loss = loss(clipped_y_pred.log(), queries_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(1.5884, grad_fn=<NllLossBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 70
    }
   ],
   "source": [
    "eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_acc = accuracy(clipped_y_pred, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Unsqueeze to force y to be of shape (K*n, 1) as this is needed for .scatter()\n",
    "\n",
    "\n",
    "# Calculated loss with negative log likelihood\n",
    "# Clip predictions for numerical stability\n",
    "eval_loss = loss(clipped_y_pred.log(), queries_labels)\n",
    "eval_acc = accuracy(clipped_y_pred, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, EPSILON):\n",
    "    # Module with cosine similarity\n",
    "\n",
    "    n_queries = queries.shape[0]\n",
    "    n_support = support.shape[0]\n",
    "\n",
    "    normalised_queries = queries / (queries.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "    normalised_support = support / (support.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "\n",
    "    expanded_x = normalised_queries.unsqueeze(1).expand(n_queries, n_support, -1)\n",
    "    expanded_y = normalised_support.unsqueeze(0).expand(n_queries, n_support, -1)\n",
    "\n",
    "    logits = (expanded_x * expanded_y).sum(dim=2)\n",
    "    return 1 - logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = (-logits(support, queries, 0.00001)).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "\n",
    "# Unsqueeze to force y to be of shape (K*n, 1) as this\n",
    "# is needed for .scatter()\n",
    "y = support_labels.unsqueeze(-1)\n",
    "y_onehot = y_onehot.scatter(1, y, 1)\n",
    "\n",
    "y_pred = torch.mm(attention, y_onehot.cuda().double())\n",
    "\n",
    "# Calculated loss with negative log likelihood\n",
    "# Clip predictions for numerical stability\n",
    "clipped_y_pred = y_pred.clamp(0.0001, 1 - 0.0001)\n",
    "#eval_loss = loss(clipped_y_pred.log(), queries_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 3, 0, 0, 3, 3, 1, 1, 1, 3, 0, 3, 1, 1, 2], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "clipped_y_pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Optionally apply full context embeddings\n",
    "    if fce:\n",
    "        # LSTM requires input of shape (seq_len, batch, input_size). `support` is of\n",
    "        # shape (k_way * n_shot, embedding_dim) and we want the LSTM to treat the\n",
    "        # support set as a sequence so add a single dimension to transform support set\n",
    "        # to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch dimension\n",
    "        # afterwards\n",
    "\n",
    "        # Calculate the fully conditional embedding, g, for support set samples as described\n",
    "        # in appendix A.2 of the paper. g takes the form of a bidirectional LSTM with a\n",
    "        # skip connection from inputs to outputs\n",
    "        support, _, _ = model.g(support.unsqueeze(1))\n",
    "        support = support.squeeze(1)\n",
    "\n",
    "        # Calculate the fully conditional embedding, f, for the query set samples as described\n",
    "        # in appendix A.1 of the paper.\n",
    "        queries = model.f(support, queries)\n",
    "\n",
    "    # Efficiently calculate distance between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = pairwise_distances(queries, support, distance)\n",
    "\n",
    "    # Calculate \"attention\" as softmax over support-query distances\n",
    "    attention = (-distances).softmax(dim=1)\n",
    "\n",
    "    # Calculate predictions as in equation (1) from Matching Networks\n",
    "    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n",
    "    y_pred = matching_net_predictions(attention, n_shot, k_way, q_queries)\n",
    "\n",
    "    # Calculated loss with negative log likelihood\n",
    "    # Clip predictions for numerical stability\n",
    "    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n",
    "    loss = loss_fn(clipped_y_pred.log(), y)\n",
    "\n",
    "    if train:\n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        # I found training to be quite unstable so I clip the norm\n",
    "        # of the gradient to be at most 1\n",
    "        clip_grad_norm_(model.parameters(), 1)\n",
    "        # Take gradient step\n",
    "        optimiser.step()\n",
    "\n",
    "    return loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, n, k, q):\n",
    "    prototypes = support.view(n, k, -1).mean(dim=1)\n",
    "    a = queries.shape[0]\n",
    "    b = prototypes.shape[0]\n",
    "    logits = -((queries.unsqueeze(1).expand(a,b,-1) - prototypes.unsqueeze(0).expand(a,b,-1))**2).sum(dim=2)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([450, 30])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "logits(support, queries, 30, 1, 15).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 4, 4, 4, 4,\n",
       "        4], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4918, 0.5899, 0.0555,  ..., 1.4189, 1.5469, 1.7144],\n",
       "        [0.1789, 0.5006, 0.4659,  ..., 1.0633, 1.3545, 0.5393],\n",
       "        [0.2796, 0.1395, 0.0412,  ..., 0.5320, 0.4916, 0.8295],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.7130, 0.5586, 0.8045],\n",
       "        [0.2357, 0.5359, 0.5710,  ..., 0.6700, 0.0000, 0.3146],\n",
       "        [0.0000, 0.0662, 0.5751,  ..., 0.6450, 0.8230, 0.8644]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = support.view(n_ways, k_shots, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 1600])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = a.shape[0]\n",
    "m = b.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]]],\n",
       "       device='cuda:0', grad_fn=<ExpandBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "queries.unsqueeze(0).expand(n,m,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[3.6123e-02, 1.1260e-01, 2.0007e-02,  ..., 7.4561e-02,\n",
       "          4.5304e-02, 2.9212e-01],\n",
       "         [2.2153e-01, 1.2597e-01, 3.0933e-02,  ..., 4.6551e-04,\n",
       "          1.0405e-01, 2.4451e-01],\n",
       "         [3.6123e-02, 1.0580e-01, 1.9951e-02,  ..., 9.2419e-01,\n",
       "          3.0241e-01, 1.6983e-01],\n",
       "         ...,\n",
       "         [2.4324e-01, 6.2327e-03, 9.3467e-06,  ..., 7.4968e-03,\n",
       "          7.1422e-01, 4.0293e-01],\n",
       "         [1.0791e-02, 1.5251e-02, 1.1098e-01,  ..., 3.5604e-02,\n",
       "          1.3514e-01, 1.5996e-02],\n",
       "         [8.2713e-02, 1.5748e-03, 2.3003e-03,  ..., 1.4747e-01,\n",
       "          1.1716e-01, 3.1607e-01]],\n",
       "\n",
       "        [[8.5715e-02, 9.8171e-02, 5.0794e-02,  ..., 1.8140e-04,\n",
       "          1.3637e-03, 2.7103e-02],\n",
       "         [1.3539e-01, 1.4225e-01, 8.4547e-03,  ..., 5.6651e-02,\n",
       "          2.4849e-01, 7.5748e-01],\n",
       "         [8.5715e-02, 9.1829e-02, 5.0705e-02,  ..., 4.9246e-01,\n",
       "          1.3987e-01, 1.3141e-03],\n",
       "         ...,\n",
       "         [1.5248e-01, 3.2159e-03, 6.5401e-03,  ..., 2.9931e-02,\n",
       "          4.4782e-01, 6.7036e-02],\n",
       "         [1.3686e-06, 2.1238e-02, 6.2101e-02,  ..., 5.0268e-03,\n",
       "          3.6745e-02, 6.2189e-02],\n",
       "         [3.4183e-02, 3.0437e-04, 1.2936e-03,  ..., 1.5481e-02,\n",
       "          2.7678e-02, 3.4726e-02]],\n",
       "\n",
       "        [[2.2964e-02, 2.4213e-01, 3.3238e-01,  ..., 3.0429e-02,\n",
       "          7.1779e-01, 3.9447e-01],\n",
       "         [2.5927e-01, 3.9368e-02, 6.7185e-02,  ..., 5.9359e-03,\n",
       "          9.7228e-02, 1.6557e-01],\n",
       "         [2.2964e-02, 2.3211e-01, 3.3215e-01,  ..., 7.4430e-01,\n",
       "          1.4026e+00, 2.4969e-01],\n",
       "         ...,\n",
       "         [2.8272e-01, 5.5437e-02, 1.8664e-01,  ..., 1.4487e-04,\n",
       "          2.1889e+00, 5.2179e-01],\n",
       "         [2.0279e-02, 1.0897e-03, 1.0393e-02,  ..., 8.1126e-03,\n",
       "          1.0040e+00, 4.5821e-02],\n",
       "         [1.0636e-01, 3.8490e-02, 1.4986e-01,  ..., 8.1449e-02,\n",
       "          9.5388e-01, 4.2222e-01]],\n",
       "\n",
       "        [[1.2849e-01, 8.3346e-02, 8.2018e-02,  ..., 1.2773e-02,\n",
       "          1.1995e-02, 7.0317e-02],\n",
       "         [9.1367e-02, 1.6143e-01, 9.5716e-04,  ..., 1.9172e-02,\n",
       "          1.8138e-01, 5.9258e-01],\n",
       "         [1.2849e-01, 7.7510e-02, 8.1904e-02,  ..., 6.4209e-01,\n",
       "          1.9944e-01, 1.8713e-02],\n",
       "         ...,\n",
       "         [1.0549e-01, 1.0293e-03, 2.0131e-02,  ..., 5.3956e-03,\n",
       "          5.5025e-01, 1.2921e-01],\n",
       "         [4.1621e-03, 2.9022e-02, 3.5415e-02,  ..., 8.2089e-04,\n",
       "          6.9846e-02, 2.2151e-02],\n",
       "         [1.4209e-02, 5.1561e-05, 9.4048e-03,  ..., 5.0164e-02,\n",
       "          5.7102e-02, 8.2307e-02]],\n",
       "\n",
       "        [[3.0861e-02, 3.8756e-02, 3.8711e-02,  ..., 3.6661e-03,\n",
       "          1.0678e-02, 1.0344e-01],\n",
       "         [2.3528e-01, 2.4365e-01, 1.4538e-02,  ..., 3.6456e-02,\n",
       "          1.8669e-01, 5.0885e-01],\n",
       "         [3.0861e-02, 3.4810e-02, 3.8633e-02,  ..., 5.6075e-01,\n",
       "          1.9396e-01, 3.7344e-02],\n",
       "         ...,\n",
       "         [2.5764e-01, 3.5700e-03, 2.7297e-03,  ..., 1.5857e-02,\n",
       "          5.4111e-01, 1.7298e-01],\n",
       "         [1.3988e-02, 6.8743e-02, 7.7187e-02,  ..., 5.6739e-04,\n",
       "          6.6615e-02, 8.5345e-03],\n",
       "         [9.1196e-02, 9.8034e-03, 5.3909e-05,  ..., 2.9413e-02,\n",
       "          5.4184e-02, 1.1788e-01]]], device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "(a.unsqueeze(1).expand(n, m, -1) - queries.unsqueeze(0).expand(n,m,-1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits = -((a.unsqueeze(1).expand(n, m, -1) -\n",
    "            b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup('omniglot', '../../omniglot', 5, 5, 5, False, 0.03, 'cuda')\n",
    "opt = optim.Adam(learner.parameters(), 0.01)\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "ttask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = ttask\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation \n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n",
    "\n",
    "# Inner adapt step\n",
    "for _ in range(1):\n",
    "    adapt_loss = loss(learner(support), support_labels)\n",
    "    learner.adapt(adapt_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-2.3058,  0.8712, -0.3009, -1.7466,  0.2174],\n",
       "        [-1.1646,  0.6933,  0.0663, -0.8611,  0.2450],\n",
       "        [-2.1604,  1.5395, -1.5346, -2.1959,  0.7367],\n",
       "        [-1.6999,  0.4818, -1.1877, -2.2231, -0.6955],\n",
       "        [-2.3995,  1.2572, -0.9198, -1.8828, -0.8643],\n",
       "        [-3.2638,  0.4875, -0.9614, -0.7871,  0.4653],\n",
       "        [-3.3043, -0.3927, -0.9587, -3.2453,  0.3632],\n",
       "        [-1.9540, -2.1205, -1.3125, -1.1369, -0.3256],\n",
       "        [-2.2095, -1.0025, -2.8084, -1.4914, -2.5339],\n",
       "        [-3.6524, -1.3129,  0.4602, -2.3179,  1.8694],\n",
       "        [-0.2180,  0.5564,  0.4151, -0.3363, -0.8863],\n",
       "        [ 3.1093, -2.3283, -1.8008,  0.9621, -2.6799],\n",
       "        [ 0.1015,  0.7979, -0.8207, -1.3333, -1.3204],\n",
       "        [ 0.5794, -1.4207, -0.9276,  0.3640, -1.8067],\n",
       "        [ 3.1537, -6.6684, -2.5053,  0.5397, -2.4689],\n",
       "        [ 1.1471, -0.7925, -0.6154, -1.3091, -1.4252],\n",
       "        [ 1.9084, -4.7103, -3.9228, -2.1826, -1.3713],\n",
       "        [ 2.2197, -0.5845, -0.1133,  0.5664, -1.8442],\n",
       "        [ 0.7250, -0.6186, -1.5460, -4.6297, -0.6061],\n",
       "        [-0.1871, -2.6361, -2.9961, -1.4738, -1.4592],\n",
       "        [ 0.1406, -0.9463,  1.4528, -0.4812, -1.1988],\n",
       "        [ 1.7844, -1.4845, -1.0468, -1.8976, -2.1174],\n",
       "        [-2.4476, -2.6299, -2.3048,  0.3068, -0.8256],\n",
       "        [-0.7030,  0.0141, -0.9806, -1.5074, -1.4905],\n",
       "        [-2.7172, -0.2624, -0.9600, -0.6464, -1.0488]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "preds = learner(queries)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = loss(preds, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    " preds.argmax(dim=1) == queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.5600, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "predictions = preds.argmax(dim=1).view(queries_labels.shape)\n",
    "(predictions == queries_labels).sum().float() / queries_labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_loss, evaluation_accuracy = inner_adapt_maml(ttask, loss, model, 5,5,5, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-373737206d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "evaluation_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-27-8ed4d768114f>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  print(p.grad.data)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8ed4d768114f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6800000071525574"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "evaluation_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.816496580927726"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.arange(3).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'anuj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../logs/{name}.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "path = '../logs/{name}.csv' \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = Profiler('ProNets_{}_{}-shot_{}-way_{}-queries'.format('omni', 5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'omni'\n",
    "prof = Profiler('abc_{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.log([2,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = Profiler('MAML_omni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.log([1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_Dataset(seed, mach, nways, kshots, tasks, data_loc):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda') if mach == 'cuda' else torch.device('cpu')\n",
    "\n",
    "    data = l2l.vision.datasets.FullOmniglot(root=data_loc, transform=transforms.Compose([\n",
    "                                                transforms.Resize(28),\n",
    "                                                transforms.ToTensor()]))\n",
    "    data = l2l.data.MetaDataset(data)\n",
    "    transform = [\n",
    "    l2l.data.transforms.NWays(data, n=2*nways),\n",
    "    l2l.data.transforms.KShots(data, k=2*kshots),\n",
    "    l2l.data.transforms.LoadData(data),\n",
    "]\n",
    "    tasksets = l2l.data.TaskDataset(data, transform, num_tasks=tasks)\n",
    "    \n",
    "    return tasksets, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasksets, data = set_Dataset(42, 'cuda', 5, 5, 20000, '../../omniglot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32460"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load train/validation/test tasksets using the benchmark interface (5-way, 5-shot)\n",
    "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
    "                                                train_ways=5,\n",
    "                                                train_samples=2*5,\n",
    "                                                test_ways=5,\n",
    "                                                test_samples=2*5,\n",
    "                                                num_tasks=20000,\n",
    "                                                root='../../omniglot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "len(tasksets.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tasksets.train.sample()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tasksets.test.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = l2l.vision.models.OmniglotFC(28 ** 2, 5)\n",
    "model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=0.5, first_order=False) #wrapper on model for in-place weight updation (adaptation)\n",
    "opt = optim.Adam(maml.parameters(), 0.003) #meta-optimizer\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MAML(\n",
       "  (module): OmniglotFC(\n",
       "    (features): Sequential(\n",
       "      (0): Flatten()\n",
       "      (1): Sequential(\n",
       "        (0): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(256, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=784, out_features=256, bias=True)\n",
       "        )\n",
       "        (1): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(128, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (2): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (3): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    data, labels = batch\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # Separate data into adaptation/evalutation sets\n",
    "    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n",
    "    adaptation_indices[np.arange(shots*ways) * 2] = True # set even indices to true\n",
    "    evaluation_indices = torch.from_numpy(~adaptation_indices)\n",
    "    adaptation_indices = torch.from_numpy(adaptation_indices)\n",
    "    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n",
    "    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        train_error = loss(learner(adaptation_data), adaptation_labels)\n",
    "        learner.adapt(train_error)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data)\n",
    "    valid_error = loss(predictions, evaluation_labels)\n",
    "    valid_accuracy = accuracy(predictions, evaluation_labels)\n",
    "    return valid_error, valid_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = maml.clone()\n",
    "batch = tasksets.train.sample()\n",
    "evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                    learner,\n",
    "                                                    loss,\n",
    "                                                    1,\n",
    "                                                    5,\n",
    "                                                    5,\n",
    "                                                    'cuda') # updates model inplace for inner update\n",
    "\n",
    "evaluation_error.backward() # gradients comp on eval set for meta-update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([256])\ntorch.Size([256])\ntorch.Size([256, 784])\ntorch.Size([256])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128, 256])\ntorch.Size([128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([5, 64])\ntorch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for p in maml.parameters():\n",
    "    a.append(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0839, -0.0497, -0.0046, -0.0065, -0.0231], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iteration in range(1):\n",
    "    opt.zero_grad()\n",
    "    meta_train_error = 0.0\n",
    "    meta_train_accuracy = 0.0\n",
    "    meta_valid_error = 0.0\n",
    "    meta_valid_accuracy = 0.0\n",
    "    for task in range(32):\n",
    "        # Compute meta-training loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.train.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device) # updates model inplace for inner update\n",
    "\n",
    "        evaluation_error.backward() # gradients comp on eval set for meta-update \n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Compute meta-validation loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.validation.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        meta_valid_error += evaluation_error.item()\n",
    "        meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "    # Print some metrics\n",
    "    print('\\n')\n",
    "    print('Iteration', iteration)\n",
    "    print('Meta Train Error', meta_train_error / meta_batch_size)\n",
    "    print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n",
    "    print('Meta Valid Error', meta_valid_error / meta_batch_size)\n",
    "    print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in maml.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()\n",
    "\n",
    "meta_test_error = 0.0\n",
    "meta_test_accuracy = 0.0\n",
    "for task in range(meta_batch_size):\n",
    "    # Compute meta-testing loss\n",
    "    learner = maml.clone()\n",
    "    batch = tasksets.test.sample()\n",
    "    evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                        learner,\n",
    "                                                        loss,\n",
    "                                                        adaptation_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)\n",
    "    meta_test_error += evaluation_error.item()\n",
    "    meta_test_accuracy += evaluation_accuracy.item()\n",
    "print('Meta Test Error', meta_test_error / meta_batch_size)\n",
    "print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(\n",
    "        ways=5,\n",
    "        shots=1,\n",
    "        meta_lr=0.003,\n",
    "        fast_lr=0.5,\n",
    "        meta_batch_size=32,\n",
    "        adaptation_steps=1,\n",
    "        num_iterations=60000,\n",
    "        cuda=True,\n",
    "        seed=42,\n",
    "):\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd08d67afa83e86333a367939f878b63191e1d5d5d165e62fc4144602f7751c67e3",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}