{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import learn2learn as l2l\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn, optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from PIL.Image import LANCZOS\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'root': '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot',\n",
       " 'n_ways': 5,\n",
       " 'k_shots': 1,\n",
       " 'q_shots': 1,\n",
       " 'inner_adapt_steps': 1,\n",
       " 'inner_lr': 0.5,\n",
       " 'meta_lr': 0.003,\n",
       " 'meta_batch_size': 32,\n",
       " 'iterations': 60000,\n",
       " 'order': False,\n",
       " 'device': 'cuda'}"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "maml_omniglot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn/src',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles',\n",
       " '/home/anuj/.vscode/extensions/ms-toolsai.jupyter-2021.6.832593372/pythonFiles/lib/python',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python38.zip',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/lib-dynload',\n",
       " '',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/datasets-1.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/locket-0.2.1-py3.8.egg',\n",
       " '/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/IPython/extensions',\n",
       " '/home/anuj/.ipython',\n",
       " '/home/anuj/Desktop/Work/TU_Delft/research/implement/learning_to_meta-learn']"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.loaders import Omniglotmix, MiniImageNet\n",
    "from data.taskers import gen_tasks\n",
    "from src.zoo.archs import EncoderNN\n",
    "#from src.zoo.maml_utils import inner_adapt_maml, setup, accuracy\n",
    "#from src.utils import Profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "classes = list(range(1623))\n",
    "random.shuffle(classes)\n",
    "image_transforms = transforms.Compose([transforms.Resize(28, interpolation=LANCZOS),\n",
    "                                                    transforms.ToTensor(),\n",
    "                                                    lambda x: 1.0 - x,\n",
    "                                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks('omniglot', '/home/anuj/Desktop/Work/TU_Delft/research/implement/omniglot', image_transforms=image_transforms, n_ways=5, k_shots=5, q_shots=5, classes=classes[:1100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "train_tasks.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = gen_tasks(dataname='miniimagenet', root='../../mini_imagenet', mode='train', n_ways=5, k_shots=1, q_shots=3, image_transforms=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1, labels1 = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 3, 84, 84])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchingNetwork(nn.Module):\n",
    "    def __init__(self, n: int, k: int, q: int, fce: bool, num_input_channels: int,\n",
    "                 lstm_layers: int, lstm_input_size: int, unrolling_steps: int, device: torch.device):\n",
    "        \"\"\"Creates a Matching Network as described in Vinyals et al.\n",
    "\n",
    "        # Arguments:\n",
    "            n: Number of examples per class in the support set\n",
    "            k: Number of classes in the few shot classification task\n",
    "            q: Number of examples per class in the query set\n",
    "            fce: Whether or not to us fully conditional embeddings\n",
    "            num_input_channels: Number of color channels the model expects input data to contain. Omniglot = 1,\n",
    "                miniImageNet = 3\n",
    "            lstm_layers: Number of LSTM layers in the bidrectional LSTM g that embeds the support set (fce = True)\n",
    "            lstm_input_size: Input size for the bidirectional and Attention LSTM. This is determined by the embedding\n",
    "                dimension of the few shot encoder which is in turn determined by the size of the input data. Hence we\n",
    "                have Omniglot -> 64, miniImageNet -> 1600.\n",
    "            unrolling_steps: Number of unrolling steps to run the Attention LSTM\n",
    "            device: Device on which to run computation\n",
    "        \"\"\"\n",
    "        super(MatchingNetwork, self).__init__()\n",
    "        self.n = n\n",
    "        self.k = k\n",
    "        self.q = q\n",
    "        self.fce = fce\n",
    "        self.num_input_channels = num_input_channels\n",
    "        self.encoder = get_few_shot_encoder(self.num_input_channels)\n",
    "        if self.fce:\n",
    "            self.g = BidrectionalLSTM(lstm_input_size, lstm_layers).to(device, dtype=torch.double)\n",
    "            self.f = AttentionLSTM(lstm_input_size, unrolling_steps=unrolling_steps).to(device, dtype=torch.double)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        pass\n",
    "\n",
    "\n",
    "class BidrectionalLSTM(nn.Module):\n",
    "    def __init__(self, size: int, layers: int):\n",
    "        \"\"\"Bidirectional LSTM used to generate fully conditional embeddings (FCE) of the support set as described\n",
    "        in the Matching Networks paper.\n",
    "\n",
    "        # Arguments\n",
    "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
    "                connection described in Appendix A.2\n",
    "            layers: Number of LSTM layers\n",
    "        \"\"\"\n",
    "        super(BidrectionalLSTM, self).__init__()\n",
    "        self.num_layers = layers\n",
    "        self.batch_size = 1\n",
    "        # Force input size and hidden size to be the same in order to implement\n",
    "        # the skip connection as described in Appendix A.1 and A.2 of Matching Networks\n",
    "        self.lstm = nn.LSTM(input_size=size,\n",
    "                            num_layers=layers,\n",
    "                            hidden_size=size,\n",
    "                            bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Give None as initial state and Pytorch LSTM creates initial hidden states\n",
    "        output, (hn, cn) = self.lstm(inputs, None)\n",
    "\n",
    "        forward_output = output[:, :, :self.lstm.hidden_size]\n",
    "        backward_output = output[:, :, self.lstm.hidden_size:]\n",
    "\n",
    "        # g(x_i, S) = h_forward_i + h_backward_i + g'(x_i) as written in Appendix A.2\n",
    "        # AKA A skip connection between inputs and outputs is used\n",
    "        output = forward_output + backward_output + inputs\n",
    "        return output, hn, cn\n",
    "\n",
    "\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, size: int, unrolling_steps: int):\n",
    "        \"\"\"Attentional LSTM used to generate fully conditional embeddings (FCE) of the query set as described\n",
    "        in the Matching Networks paper.\n",
    "\n",
    "        # Arguments\n",
    "            size: Size of input and hidden layers. These are constrained to be the same in order to implement the skip\n",
    "                connection described in Appendix A.2\n",
    "            unrolling_steps: Number of steps of attention over the support set to compute. Analogous to number of\n",
    "                layers in a regular LSTM\n",
    "        \"\"\"\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.unrolling_steps = unrolling_steps\n",
    "        self.lstm_cell = nn.LSTMCell(input_size=size,\n",
    "                                     hidden_size=size).double()\n",
    "\n",
    "    def forward(self, support, queries):\n",
    "        # Get embedding dimension, d\n",
    "        if support.shape[-1] != queries.shape[-1]:\n",
    "            raise(ValueError(\"Support and query set have different embedding dimension!\"))\n",
    "\n",
    "        batch_size = queries.shape[0]\n",
    "        embedding_dim = queries.shape[1]\n",
    "\n",
    "        h_hat = torch.zeros_like(queries).cuda().double()\n",
    "        c = torch.zeros(batch_size, embedding_dim).cuda().double()\n",
    "\n",
    "        for k in range(self.unrolling_steps):\n",
    "            # Calculate hidden state cf. equation (4) of appendix A.2\n",
    "            h = h_hat + queries\n",
    "\n",
    "            # Calculate softmax attentions between hidden states and support set embeddings\n",
    "            # cf. equation (6) of appendix A.2\n",
    "            attentions = torch.mm(h.double(), support.t().double())\n",
    "            attentions = attentions.softmax(dim=1)\n",
    "\n",
    "            # Calculate readouts from support set embeddings cf. equation (5)\n",
    "            readout = torch.mm(attentions.double(), support.double())\n",
    "\n",
    "            # Run LSTM cell cf. equation (3)\n",
    "            # h_hat, c = self.lstm_cell(queries, (torch.cat([h, readout], dim=1), c))\n",
    "            h_hat, c = self.lstm_cell(queries.double(), (h.double() + readout.double(), c.double()))\n",
    "\n",
    "        h = h_hat + queries\n",
    "\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EncoderNN(3, (2,2), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = model(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20, 1600])"
      ]
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "labels1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "n_ways = 5; k_shots = 1; q_shots = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = a, labels1\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation\n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(\n",
    "        k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([15, 1600])"
      ]
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "queries.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.0717, 0.1103, 0.1152,  ..., 0.0000, 0.0000, 0.0000]],\n",
       "\n",
       "        [[0.3320, 0.1983, 0.0410,  ..., 0.3116, 0.3548, 0.0000]],\n",
       "\n",
       "        [[0.0693, 0.1728, 0.1736,  ..., 0.0000, 0.0000, 0.0394]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0000,  ..., 0.5983, 0.5914, 0.3825]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.0132,  ..., 0.1370, 0.0000, 0.0000]]],\n",
       "       device='cuda:0', grad_fn=<UnsqueezeBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "support.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = BidrectionalLSTM(1600, 1).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "support, _, _ = g(support.unsqueeze(1))\n",
    "support = support.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1885,  0.5725,  0.2545,  ...,  0.6749,  0.4760,  0.4216],\n",
       "        [ 0.1941,  0.3804,  0.4836,  ...,  0.1441,  0.7202,  1.3960],\n",
       "        [ 0.1631,  0.4248,  0.3174,  ...,  0.8357,  0.0433,  2.3977],\n",
       "        [ 0.2788,  1.0453,  0.7681,  ...,  0.6483,  0.3816,  0.6864],\n",
       "        [ 0.7467,  0.7731,  0.5384,  ..., -0.1106,  0.2103,  0.1904]],\n",
       "       device='cuda:0', grad_fn=<SqueezeBackward1>)"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = AttentionLSTM(1600, 2).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = f(support, queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, EPSILON):\n",
    "    # Module with cosine similarity\n",
    "\n",
    "    n_queries = queries.shape[0]\n",
    "    n_support = support.shape[0]\n",
    "\n",
    "    normalised_queries = queries / (queries.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "    normalised_support = support / (support.pow(2).sum(dim=1, keepdim=True).sqrt() + EPSILON)\n",
    "\n",
    "    expanded_x = normalised_queries.unsqueeze(1).expand(n_queries, n_support, -1)\n",
    "    expanded_y = normalised_support.unsqueeze(0).expand(n_queries, n_support, -1)\n",
    "\n",
    "    logits = (expanded_x * expanded_y).sum(dim=2)\n",
    "    return 1 - logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention = (-logits(support, queries, 0.00001)).softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_onehot = torch.zeros(n_ways * k_shots, n_ways).to(device)\n",
    "\n",
    "# Unsqueeze to force y to be of shape (K*n, 1) as this\n",
    "# is needed for .scatter()\n",
    "y = support_labels.unsqueeze(-1)\n",
    "y_onehot = y_onehot.scatter(1, y, 1)\n",
    "\n",
    "y_pred = torch.mm(attention, y_onehot.cuda().double())\n",
    "\n",
    "# Calculated loss with negative log likelihood\n",
    "# Clip predictions for numerical stability\n",
    "clipped_y_pred = y_pred.clamp(0.0001, 1 - 0.0001)\n",
    "#eval_loss = loss(clipped_y_pred.log(), queries_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([0, 3, 0, 0, 3, 3, 1, 1, 1, 3, 0, 3, 1, 1, 2], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "clipped_y_pred.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    # Optionally apply full context embeddings\n",
    "    if fce:\n",
    "        # LSTM requires input of shape (seq_len, batch, input_size). `support` is of\n",
    "        # shape (k_way * n_shot, embedding_dim) and we want the LSTM to treat the\n",
    "        # support set as a sequence so add a single dimension to transform support set\n",
    "        # to the shape (k_way * n_shot, 1, embedding_dim) and then remove the batch dimension\n",
    "        # afterwards\n",
    "\n",
    "        # Calculate the fully conditional embedding, g, for support set samples as described\n",
    "        # in appendix A.2 of the paper. g takes the form of a bidirectional LSTM with a\n",
    "        # skip connection from inputs to outputs\n",
    "        support, _, _ = model.g(support.unsqueeze(1))\n",
    "        support = support.squeeze(1)\n",
    "\n",
    "        # Calculate the fully conditional embedding, f, for the query set samples as described\n",
    "        # in appendix A.1 of the paper.\n",
    "        queries = model.f(support, queries)\n",
    "\n",
    "    # Efficiently calculate distance between all queries and all prototypes\n",
    "    # Output should have shape (q_queries * k_way, k_way) = (num_queries, k_way)\n",
    "    distances = pairwise_distances(queries, support, distance)\n",
    "\n",
    "    # Calculate \"attention\" as softmax over support-query distances\n",
    "    attention = (-distances).softmax(dim=1)\n",
    "\n",
    "    # Calculate predictions as in equation (1) from Matching Networks\n",
    "    # y_hat = \\sum_{i=1}^{k} a(x_hat, x_i) y_i\n",
    "    y_pred = matching_net_predictions(attention, n_shot, k_way, q_queries)\n",
    "\n",
    "    # Calculated loss with negative log likelihood\n",
    "    # Clip predictions for numerical stability\n",
    "    clipped_y_pred = y_pred.clamp(EPSILON, 1 - EPSILON)\n",
    "    loss = loss_fn(clipped_y_pred.log(), y)\n",
    "\n",
    "    if train:\n",
    "        # Backpropagate gradients\n",
    "        loss.backward()\n",
    "        # I found training to be quite unstable so I clip the norm\n",
    "        # of the gradient to be at most 1\n",
    "        clip_grad_norm_(model.parameters(), 1)\n",
    "        # Take gradient step\n",
    "        optimiser.step()\n",
    "\n",
    "    return loss, y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits(support, queries, n, k, q):\n",
    "    prototypes = support.view(n, k, -1).mean(dim=1)\n",
    "    a = queries.shape[0]\n",
    "    b = prototypes.shape[0]\n",
    "    logits = -((queries.unsqueeze(1).expand(a,b,-1) - prototypes.unsqueeze(0).expand(a,b,-1))**2).sum(dim=2)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support.view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([450, 30])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "logits(support, queries, 30, 1, 15).shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 2, 2, 2, 2, 2, 4, 4, 4, 4,\n",
       "        4], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.4918, 0.5899, 0.0555,  ..., 1.4189, 1.5469, 1.7144],\n",
       "        [0.1789, 0.5006, 0.4659,  ..., 1.0633, 1.3545, 0.5393],\n",
       "        [0.2796, 0.1395, 0.0412,  ..., 0.5320, 0.4916, 0.8295],\n",
       "        ...,\n",
       "        [0.0000, 0.0000, 0.0000,  ..., 0.7130, 0.5586, 0.8045],\n",
       "        [0.2357, 0.5359, 0.5710,  ..., 0.6700, 0.0000, 0.3146],\n",
       "        [0.0000, 0.0662, 0.5751,  ..., 0.6450, 0.8230, 0.8644]],\n",
       "       device='cuda:0', grad_fn=<IndexBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = support.view(n_ways, k_shots, -1).mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([5, 1600])"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = a.shape[0]\n",
    "m = b.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([100, 128])"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input1 = torch.randn(100, 128)\n",
    "input2 = torch.randn(100, 128)\n",
    "cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "output = cos(input1, input2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]],\n",
       "\n",
       "        [[0.0000, 0.0000, 0.1363,  ..., 0.7791, 0.6836, 0.4011],\n",
       "         [0.6607, 0.6905, 0.4536,  ..., 1.0306, 1.2191, 1.4360],\n",
       "         [0.0000, 0.0103, 0.1365,  ..., 0.0908, 0.3466, 0.5294],\n",
       "         ...,\n",
       "         [0.6833, 0.2566, 0.2808,  ..., 0.9656, 0.0514, 0.3068],\n",
       "         [0.2939, 0.4591, 0.6109,  ..., 0.8635, 0.5289, 0.8151],\n",
       "         [0.4777, 0.2959, 0.3257,  ..., 0.6681, 0.5542, 0.3793]]],\n",
       "       device='cuda:0', grad_fn=<ExpandBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 60
    }
   ],
   "source": [
    "queries.unsqueeze(0).expand(n,m,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[3.6123e-02, 1.1260e-01, 2.0007e-02,  ..., 7.4561e-02,\n",
       "          4.5304e-02, 2.9212e-01],\n",
       "         [2.2153e-01, 1.2597e-01, 3.0933e-02,  ..., 4.6551e-04,\n",
       "          1.0405e-01, 2.4451e-01],\n",
       "         [3.6123e-02, 1.0580e-01, 1.9951e-02,  ..., 9.2419e-01,\n",
       "          3.0241e-01, 1.6983e-01],\n",
       "         ...,\n",
       "         [2.4324e-01, 6.2327e-03, 9.3467e-06,  ..., 7.4968e-03,\n",
       "          7.1422e-01, 4.0293e-01],\n",
       "         [1.0791e-02, 1.5251e-02, 1.1098e-01,  ..., 3.5604e-02,\n",
       "          1.3514e-01, 1.5996e-02],\n",
       "         [8.2713e-02, 1.5748e-03, 2.3003e-03,  ..., 1.4747e-01,\n",
       "          1.1716e-01, 3.1607e-01]],\n",
       "\n",
       "        [[8.5715e-02, 9.8171e-02, 5.0794e-02,  ..., 1.8140e-04,\n",
       "          1.3637e-03, 2.7103e-02],\n",
       "         [1.3539e-01, 1.4225e-01, 8.4547e-03,  ..., 5.6651e-02,\n",
       "          2.4849e-01, 7.5748e-01],\n",
       "         [8.5715e-02, 9.1829e-02, 5.0705e-02,  ..., 4.9246e-01,\n",
       "          1.3987e-01, 1.3141e-03],\n",
       "         ...,\n",
       "         [1.5248e-01, 3.2159e-03, 6.5401e-03,  ..., 2.9931e-02,\n",
       "          4.4782e-01, 6.7036e-02],\n",
       "         [1.3686e-06, 2.1238e-02, 6.2101e-02,  ..., 5.0268e-03,\n",
       "          3.6745e-02, 6.2189e-02],\n",
       "         [3.4183e-02, 3.0437e-04, 1.2936e-03,  ..., 1.5481e-02,\n",
       "          2.7678e-02, 3.4726e-02]],\n",
       "\n",
       "        [[2.2964e-02, 2.4213e-01, 3.3238e-01,  ..., 3.0429e-02,\n",
       "          7.1779e-01, 3.9447e-01],\n",
       "         [2.5927e-01, 3.9368e-02, 6.7185e-02,  ..., 5.9359e-03,\n",
       "          9.7228e-02, 1.6557e-01],\n",
       "         [2.2964e-02, 2.3211e-01, 3.3215e-01,  ..., 7.4430e-01,\n",
       "          1.4026e+00, 2.4969e-01],\n",
       "         ...,\n",
       "         [2.8272e-01, 5.5437e-02, 1.8664e-01,  ..., 1.4487e-04,\n",
       "          2.1889e+00, 5.2179e-01],\n",
       "         [2.0279e-02, 1.0897e-03, 1.0393e-02,  ..., 8.1126e-03,\n",
       "          1.0040e+00, 4.5821e-02],\n",
       "         [1.0636e-01, 3.8490e-02, 1.4986e-01,  ..., 8.1449e-02,\n",
       "          9.5388e-01, 4.2222e-01]],\n",
       "\n",
       "        [[1.2849e-01, 8.3346e-02, 8.2018e-02,  ..., 1.2773e-02,\n",
       "          1.1995e-02, 7.0317e-02],\n",
       "         [9.1367e-02, 1.6143e-01, 9.5716e-04,  ..., 1.9172e-02,\n",
       "          1.8138e-01, 5.9258e-01],\n",
       "         [1.2849e-01, 7.7510e-02, 8.1904e-02,  ..., 6.4209e-01,\n",
       "          1.9944e-01, 1.8713e-02],\n",
       "         ...,\n",
       "         [1.0549e-01, 1.0293e-03, 2.0131e-02,  ..., 5.3956e-03,\n",
       "          5.5025e-01, 1.2921e-01],\n",
       "         [4.1621e-03, 2.9022e-02, 3.5415e-02,  ..., 8.2089e-04,\n",
       "          6.9846e-02, 2.2151e-02],\n",
       "         [1.4209e-02, 5.1561e-05, 9.4048e-03,  ..., 5.0164e-02,\n",
       "          5.7102e-02, 8.2307e-02]],\n",
       "\n",
       "        [[3.0861e-02, 3.8756e-02, 3.8711e-02,  ..., 3.6661e-03,\n",
       "          1.0678e-02, 1.0344e-01],\n",
       "         [2.3528e-01, 2.4365e-01, 1.4538e-02,  ..., 3.6456e-02,\n",
       "          1.8669e-01, 5.0885e-01],\n",
       "         [3.0861e-02, 3.4810e-02, 3.8633e-02,  ..., 5.6075e-01,\n",
       "          1.9396e-01, 3.7344e-02],\n",
       "         ...,\n",
       "         [2.5764e-01, 3.5700e-03, 2.7297e-03,  ..., 1.5857e-02,\n",
       "          5.4111e-01, 1.7298e-01],\n",
       "         [1.3988e-02, 6.8743e-02, 7.7187e-02,  ..., 5.6739e-04,\n",
       "          6.6615e-02, 8.5345e-03],\n",
       "         [9.1196e-02, 9.8034e-03, 5.3909e-05,  ..., 2.9413e-02,\n",
       "          5.4184e-02, 1.1788e-01]]], device='cuda:0', grad_fn=<PowBackward0>)"
      ]
     },
     "metadata": {},
     "execution_count": 61
    }
   ],
   "source": [
    "(a.unsqueeze(1).expand(n, m, -1) - queries.unsqueeze(0).expand(n,m,-1))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logits = -((a.unsqueeze(1).expand(n, m, -1) -\n",
    "            b.unsqueeze(0).expand(n, m, -1))**2).sum(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "train_tasks, valid_tasks, test_tasks, learner = setup('omniglot', '../../omniglot', 5, 5, 5, False, 0.03, 'cuda')\n",
    "opt = optim.Adam(learner.parameters(), 0.01)\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ttask = train_tasks.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "ttask[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = learner.clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, labels = ttask\n",
    "data, labels = data.to(device), labels.to(device)\n",
    "total = n_ways * (k_shots + q_shots)\n",
    "queries_index = np.zeros(total)\n",
    "\n",
    "# Extracting the evaluation datums from the entire task set, for the meta gradient calculation \n",
    "for offset in range(n_ways):\n",
    "    queries_index[np.random.choice(k_shots+q_shots, q_shots, replace=False) + ((k_shots + q_shots)*offset)] = True\n",
    "support = data[np.where(queries_index == 0)]\n",
    "support_labels = labels[np.where(queries_index == 0)]\n",
    "queries = data[np.where(queries_index == 1)]\n",
    "queries_labels = labels[np.where(queries_index == 1)]\n",
    "\n",
    "# Inner adapt step\n",
    "for _ in range(1):\n",
    "    adapt_loss = loss(learner(support), support_labels)\n",
    "    learner.adapt(adapt_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-2.3058,  0.8712, -0.3009, -1.7466,  0.2174],\n",
       "        [-1.1646,  0.6933,  0.0663, -0.8611,  0.2450],\n",
       "        [-2.1604,  1.5395, -1.5346, -2.1959,  0.7367],\n",
       "        [-1.6999,  0.4818, -1.1877, -2.2231, -0.6955],\n",
       "        [-2.3995,  1.2572, -0.9198, -1.8828, -0.8643],\n",
       "        [-3.2638,  0.4875, -0.9614, -0.7871,  0.4653],\n",
       "        [-3.3043, -0.3927, -0.9587, -3.2453,  0.3632],\n",
       "        [-1.9540, -2.1205, -1.3125, -1.1369, -0.3256],\n",
       "        [-2.2095, -1.0025, -2.8084, -1.4914, -2.5339],\n",
       "        [-3.6524, -1.3129,  0.4602, -2.3179,  1.8694],\n",
       "        [-0.2180,  0.5564,  0.4151, -0.3363, -0.8863],\n",
       "        [ 3.1093, -2.3283, -1.8008,  0.9621, -2.6799],\n",
       "        [ 0.1015,  0.7979, -0.8207, -1.3333, -1.3204],\n",
       "        [ 0.5794, -1.4207, -0.9276,  0.3640, -1.8067],\n",
       "        [ 3.1537, -6.6684, -2.5053,  0.5397, -2.4689],\n",
       "        [ 1.1471, -0.7925, -0.6154, -1.3091, -1.4252],\n",
       "        [ 1.9084, -4.7103, -3.9228, -2.1826, -1.3713],\n",
       "        [ 2.2197, -0.5845, -0.1133,  0.5664, -1.8442],\n",
       "        [ 0.7250, -0.6186, -1.5460, -4.6297, -0.6061],\n",
       "        [-0.1871, -2.6361, -2.9961, -1.4738, -1.4592],\n",
       "        [ 0.1406, -0.9463,  1.4528, -0.4812, -1.1988],\n",
       "        [ 1.7844, -1.4845, -1.0468, -1.8976, -2.1174],\n",
       "        [-2.4476, -2.6299, -2.3048,  0.3068, -0.8256],\n",
       "        [-0.7030,  0.0141, -0.9806, -1.5074, -1.4905],\n",
       "        [-2.7172, -0.2624, -0.9600, -0.6464, -1.0488]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "preds = learner(queries)\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_loss = loss(preds, queries_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True,  True, False,  True,  True, False,  True,\n",
       "        False, False, False, False, False,  True,  True,  True,  True,  True,\n",
       "         True, False, False, False, False], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    " preds.argmax(dim=1) == queries_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(0.5600, device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "predictions = preds.argmax(dim=1).view(queries_labels.shape)\n",
    "(predictions == queries_labels).sum().float() / queries_labels.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_loss, evaluation_accuracy = inner_adapt_maml(ttask, loss, model, 5,5,5, 1, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-373737206d5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluation_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/torch/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "evaluation_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "<ipython-input-27-8ed4d768114f>:2: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n  print(p.grad.data)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-8ed4d768114f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "for p in model.parameters():\n",
    "    print(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.6800000071525574"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "evaluation_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.816496580927726"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "np.arange(3).std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'anuj'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'../logs/{name}.csv'"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "path = '../logs/{name}.csv' \n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler = Profiler('ProNets_{}_{}-shot_{}-way_{}-queries'.format('omni', 5,5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = 'omni'\n",
    "prof = Profiler('abc_{}'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiler.log([2,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof = Profiler('MAML_omni')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof.log([1,2,3,4,5,6,7,8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predictions = predictions.argmax(dim=1).view(targets.shape)\n",
    "    return (predictions == targets).sum().float() / targets.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_Dataset(seed, mach, nways, kshots, tasks, data_loc):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    device = torch.device('cuda') if mach == 'cuda' else torch.device('cpu')\n",
    "\n",
    "    data = l2l.vision.datasets.FullOmniglot(root=data_loc, transform=transforms.Compose([\n",
    "                                                transforms.Resize(28),\n",
    "                                                transforms.ToTensor()]))\n",
    "    data = l2l.data.MetaDataset(data)\n",
    "    transform = [\n",
    "    l2l.data.transforms.NWays(data, n=2*nways),\n",
    "    l2l.data.transforms.KShots(data, k=2*kshots),\n",
    "    l2l.data.transforms.LoadData(data),\n",
    "]\n",
    "    tasksets = l2l.data.TaskDataset(data, transform, num_tasks=tasks)\n",
    "    \n",
    "    return tasksets, data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasksets, data = set_Dataset(42, 'cuda', 5, 5, 20000, '../../omniglot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "32460"
      ]
     },
     "metadata": {},
     "execution_count": 157
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/anuj/anaconda3/envs/torch/lib/python3.8/site-packages/torchvision/transforms/transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load train/validation/test tasksets using the benchmark interface (5-way, 5-shot)\n",
    "tasksets = l2l.vision.benchmarks.get_tasksets('omniglot',\n",
    "                                                train_ways=5,\n",
    "                                                train_samples=2*5,\n",
    "                                                test_ways=5,\n",
    "                                                test_samples=2*5,\n",
    "                                                num_tasks=20000,\n",
    "                                                root='../../omniglot',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "metadata": {},
     "execution_count": 161
    }
   ],
   "source": [
    "len(tasksets.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "tasksets.train.sample()[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([50, 1, 28, 28])"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "tasksets.test.sample()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = l2l.vision.models.OmniglotFC(28 ** 2, 5)\n",
    "model.to(device)\n",
    "maml = l2l.algorithms.MAML(model, lr=0.5, first_order=False) #wrapper on model for in-place weight updation (adaptation)\n",
    "opt = optim.Adam(maml.parameters(), 0.003) #meta-optimizer\n",
    "loss = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "MAML(\n",
       "  (module): OmniglotFC(\n",
       "    (features): Sequential(\n",
       "      (0): Flatten()\n",
       "      (1): Sequential(\n",
       "        (0): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(256, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=784, out_features=256, bias=True)\n",
       "        )\n",
       "        (1): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(128, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=256, out_features=128, bias=True)\n",
       "        )\n",
       "        (2): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=True)\n",
       "        )\n",
       "        (3): LinearBlock(\n",
       "          (relu): ReLU()\n",
       "          (normalize): BatchNorm1d(64, eps=0.001, momentum=0.999, affine=True, track_running_stats=False)\n",
       "          (linear): Linear(in_features=64, out_features=64, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (classifier): Linear(in_features=64, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 49
    }
   ],
   "source": [
    "maml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fast_adapt(batch, learner, loss, adaptation_steps, shots, ways, device):\n",
    "    data, labels = batch\n",
    "    data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "    # Separate data into adaptation/evalutation sets\n",
    "    adaptation_indices = np.zeros(data.size(0), dtype=bool)\n",
    "    adaptation_indices[np.arange(shots*ways) * 2] = True # set even indices to true\n",
    "    evaluation_indices = torch.from_numpy(~adaptation_indices)\n",
    "    adaptation_indices = torch.from_numpy(adaptation_indices)\n",
    "    adaptation_data, adaptation_labels = data[adaptation_indices], labels[adaptation_indices]\n",
    "    evaluation_data, evaluation_labels = data[evaluation_indices], labels[evaluation_indices]\n",
    "\n",
    "    # Adapt the model\n",
    "    for step in range(adaptation_steps):\n",
    "        train_error = loss(learner(adaptation_data), adaptation_labels)\n",
    "        learner.adapt(train_error)\n",
    "\n",
    "    # Evaluate the adapted model\n",
    "    predictions = learner(evaluation_data)\n",
    "    valid_error = loss(predictions, evaluation_labels)\n",
    "    valid_accuracy = accuracy(predictions, evaluation_labels)\n",
    "    return valid_error, valid_accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = maml.clone()\n",
    "batch = tasksets.train.sample()\n",
    "evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                    learner,\n",
    "                                                    loss,\n",
    "                                                    1,\n",
    "                                                    5,\n",
    "                                                    5,\n",
    "                                                    'cuda') # updates model inplace for inner update\n",
    "\n",
    "evaluation_error.backward() # gradients comp on eval set for meta-update "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([256])\ntorch.Size([256])\ntorch.Size([256, 784])\ntorch.Size([256])\ntorch.Size([128])\ntorch.Size([128])\ntorch.Size([128, 256])\ntorch.Size([128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 128])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64])\ntorch.Size([64, 64])\ntorch.Size([64])\ntorch.Size([5, 64])\ntorch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "a=[]\n",
    "for p in maml.parameters():\n",
    "    a.append(p.grad.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([ 0.0839, -0.0497, -0.0046, -0.0065, -0.0231], device='cuda:0')"
      ]
     },
     "metadata": {},
     "execution_count": 94
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iteration in range(1):\n",
    "    opt.zero_grad()\n",
    "    meta_train_error = 0.0\n",
    "    meta_train_accuracy = 0.0\n",
    "    meta_valid_error = 0.0\n",
    "    meta_valid_accuracy = 0.0\n",
    "    for task in range(32):\n",
    "        # Compute meta-training loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.train.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device) # updates model inplace for inner update\n",
    "\n",
    "        evaluation_error.backward() # gradients comp on eval set for meta-update \n",
    "        meta_train_error += evaluation_error.item()\n",
    "        meta_train_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "        # Compute meta-validation loss\n",
    "        learner = maml.clone()\n",
    "        batch = tasksets.validation.sample()\n",
    "        evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                            learner,\n",
    "                                                            loss,\n",
    "                                                            adaptation_steps,\n",
    "                                                            shots,\n",
    "                                                            ways,\n",
    "                                                            device)\n",
    "        meta_valid_error += evaluation_error.item()\n",
    "        meta_valid_accuracy += evaluation_accuracy.item()\n",
    "\n",
    "    # Print some metrics\n",
    "    print('\\n')\n",
    "    print('Iteration', iteration)\n",
    "    print('Meta Train Error', meta_train_error / meta_batch_size)\n",
    "    print('Meta Train Accuracy', meta_train_accuracy / meta_batch_size)\n",
    "    print('Meta Valid Error', meta_valid_error / meta_batch_size)\n",
    "    print('Meta Valid Accuracy', meta_valid_accuracy / meta_batch_size)\n",
    "\n",
    "    # Average the accumulated gradients and optimize\n",
    "    for p in maml.parameters():\n",
    "        p.grad.data.mul_(1.0 / meta_batch_size)\n",
    "    opt.step()\n",
    "\n",
    "meta_test_error = 0.0\n",
    "meta_test_accuracy = 0.0\n",
    "for task in range(meta_batch_size):\n",
    "    # Compute meta-testing loss\n",
    "    learner = maml.clone()\n",
    "    batch = tasksets.test.sample()\n",
    "    evaluation_error, evaluation_accuracy = fast_adapt(batch,\n",
    "                                                        learner,\n",
    "                                                        loss,\n",
    "                                                        adaptation_steps,\n",
    "                                                        shots,\n",
    "                                                        ways,\n",
    "                                                        device)\n",
    "    meta_test_error += evaluation_error.item()\n",
    "    meta_test_accuracy += evaluation_accuracy.item()\n",
    "print('Meta Test Error', meta_test_error / meta_batch_size)\n",
    "print('Meta Test Accuracy', meta_test_accuracy / meta_batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main(\n",
    "        ways=5,\n",
    "        shots=1,\n",
    "        meta_lr=0.003,\n",
    "        fast_lr=0.5,\n",
    "        meta_batch_size=32,\n",
    "        adaptation_steps=1,\n",
    "        num_iterations=60000,\n",
    "        cuda=True,\n",
    "        seed=42,\n",
    "):\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd08d67afa83e86333a367939f878b63191e1d5d5d165e62fc4144602f7751c67e3",
   "display_name": "Python 3.8.8 64-bit ('torch': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}